{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blindspotm/fine-tuning/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAyD4AISk-dL"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n",
        "</div>\n",
        "\n",
        "To install Unsloth your local device, follow [our guide](https://docs.unsloth.ai/get-started/install-and-update). This notebook is licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zaJRErbk-dM"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax-nO285k-dM"
      },
      "source": [
        "\n",
        "Introducing FP8 precision training for faster RL inference. [Read Blog](https://docs.unsloth.ai/new/fp8-reinforcement-learning).\n",
        "\n",
        "Unsloth's [Docker image](https://hub.docker.com/r/unsloth/unsloth) is here! Start training with no setup & environment issues. [Read our Guide](https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker).\n",
        "\n",
        "[gpt-oss RL](https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning) is now supported with the fastest inference & lowest VRAM. Try our [new notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb) which creates kernels!\n",
        "\n",
        "Introducing [Vision](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl) and [Standby](https://docs.unsloth.ai/basics/memory-efficient-rl) for RL! Train Qwen, Gemma etc. VLMs with GSPO - even faster with less VRAM.\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElbyBQIdk-dN"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtysP0lsk-dN"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.56.2\n",
        "!pip install --no-deps trl==0.22.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfN7_q30k-dN"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmUBVEnvCDJv"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
        "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
        "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
        "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
        "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep\n",
        "We now use the Alpaca dataset from [yahma](https://huggingface.co/datasets/yahma/alpaca-cleaned), which is a filtered version of 52K of the original [Alpaca dataset](https://crfm.stanford.edu/2023/03/13/alpaca.html). You can replace this code section with your own data prep.\n",
        "\n",
        "**[NOTE]** To train only on completions (ignoring the user's input) read TRL's docs [here](https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only).\n",
        "\n",
        "**[NOTE]** Remember to add the **EOS_TOKEN** to the tokenized output!! Otherwise you'll get infinite generations!\n",
        "\n",
        "If you want to use the `llama-3` template for ShareGPT datasets, try our conversational [notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Alpaca.ipynb)\n",
        "\n",
        "For text completions like novel writing, try this [notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_(7B)-Text_Completion.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's train our model. We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.001,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use TrackIO/WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2ejIt2xSNKKp"
      },
      "outputs": [],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pCqnaKmlO1U9"
      },
      "outputs": [],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! You can change the instruction and input - leave the output blank!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR3gIAX-SM2q"
      },
      "outputs": [],
      "source": [
        "# alpaca_prompt = Copied from above\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"Continue the fibonnaci sequence.\", # instruction\n",
        "        \"1, 1, 2, 3, 5, 8\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrSvZObor0lY"
      },
      "source": [
        " You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2pEuRb1r2Vg"
      },
      "outputs": [],
      "source": [
        "# alpaca_prompt = Copied from above\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"Continue the fibonnaci sequence.\", # instruction\n",
        "        \"1, 1, 2, 3, 5, 8\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"lora_model\")  # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKX_XKs_BNZR"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "# alpaca_prompt = You MUST copy from above!\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"What is a famous tall tower in Paris?\", # instruction\n",
        "        \"\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQMjaNrjsU5_"
      },
      "source": [
        "You can also use Hugging Face's `AutoModelForPeftCausalLM`. Only use this if you do not have `unsloth` installed. It can be hopelessly slow, since `4bit` model downloading is not supported, and Unsloth's **inference is 2x faster**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFfaXG0WsQuE"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    # I highly do NOT suggest - use Unsloth if possible\n",
        "    from peft import AutoPeftModelForCausalLM\n",
        "    from transformers import AutoTokenizer\n",
        "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "        \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False:\n",
        "    model.save_pretrained(\"model\")\n",
        "    tokenizer.save_pretrained(\"model\")\n",
        "if False:\n",
        "    model.push_to_hub(\"hf/model\", token = \"\")\n",
        "    tokenizer.push_to_hub(\"hf/model\", token = \"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCv4vXHd61i7"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
        "\n",
        "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
        "\n",
        "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqfebeAdT073"
      },
      "outputs": [],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
        "# And change hf to your username!\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"hf/model\", # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = \"\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrE00In2k-dQ"
      },
      "source": [
        "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp.\n",
        "\n",
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ⭐️ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐️\n",
        "\n",
        "  This notebook and all Unsloth notebooks are licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d229009d"
      },
      "source": [
        "# Task\n",
        "Load the first 5 lines from \"/content/chatgpt_raw.jsonl\" and \"/content/claude_raw.jsonl\". Parse the JSON objects to inspect the keys and print the content of these entries, focusing on the instructions and responses to compare formatting and content depth. Finally, summarize the findings regarding formatting differences and content quality to recommend a base model for the master file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86495ed0"
      },
      "source": [
        "## Load and Inspect Data\n",
        "\n",
        "### Subtask:\n",
        "Load the first 5 lines from the raw JSONL files and inspect their structure.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cd776b4"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the first 5 lines from the raw JSONL files and inspect their structure as requested.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a90b514"
      },
      "source": [
        "import json\n",
        "\n",
        "# Load first 5 lines from chatgpt_raw.jsonl\n",
        "chatgpt_entries = []\n",
        "with open('/content/chatgpt_raw.jsonl', 'r') as f:\n",
        "    for _ in range(5):\n",
        "        line = f.readline()\n",
        "        if not line: break\n",
        "        chatgpt_entries.append(json.loads(line))\n",
        "\n",
        "# Load first 5 lines from claude_raw.jsonl\n",
        "claude_entries = []\n",
        "with open('/content/claude_raw.jsonl', 'r') as f:\n",
        "    for _ in range(5):\n",
        "        line = f.readline()\n",
        "        if not line: break\n",
        "        claude_entries.append(json.loads(line))\n",
        "\n",
        "# Inspect structure\n",
        "if chatgpt_entries:\n",
        "    print(\"ChatGPT Data Keys:\", chatgpt_entries[0].keys())\n",
        "else:\n",
        "    print(\"ChatGPT file is empty or could not be read.\")\n",
        "\n",
        "if claude_entries:\n",
        "    print(\"Claude Data Keys:\", claude_entries[0].keys())\n",
        "else:\n",
        "    print(\"Claude file is empty or could not be read.\")\n",
        "\n",
        "print(\"\\nChatGPT Entries (First 5):\")\n",
        "print(chatgpt_entries)\n",
        "\n",
        "print(\"\\nClaude Entries (First 5):\")\n",
        "print(claude_entries)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2efe1d2c"
      },
      "source": [
        "# Task\n",
        "Validate the structure of every line in \"/content/chatgpt_raw.jsonl\" and \"/content/claude_raw.jsonl\" to ensure they contain the keys 'instruction', 'input', and 'output'. Concurrently, analyze the 'output' text to count specific 'AI tells' (e.g., 'As an AI', 'Certainly', 'Here is', 'limitations') and 'Marketing Frameworks' (e.g., 'ROAS', 'CPA', 'Contribution Margin', 'guardrails'). Report the validation status and compare the phrase frequencies between the two datasets to formulate a data merging strategy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfcb453f"
      },
      "source": [
        "## Validate Structure and Keys\n",
        "\n",
        "### Subtask:\n",
        "Iterate through every line of 'chatgpt_raw.jsonl' and 'claude_raw.jsonl'. Verify that every entry is valid JSON and contains exactly the keys 'instruction', 'input', and 'output'. Report any malformed lines or missing keys.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2760c372"
      },
      "source": [
        "**Reasoning**:\n",
        "I will write a script to iterate through 'chatgpt_raw.jsonl' and 'claude_raw.jsonl', verifying that each line is valid JSON and contains the required keys ('instruction', 'input', 'output'), and then print a summary of valid lines and any errors found.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f44c0ca"
      },
      "source": [
        "import json\n",
        "\n",
        "file_paths = ['/content/chatgpt_raw.jsonl', '/content/claude_raw.jsonl']\n",
        "required_keys = {'instruction', 'input', 'output'}\n",
        "\n",
        "for file_path in file_paths:\n",
        "    print(f\"Checking file: {file_path}\")\n",
        "    total_lines = 0\n",
        "    valid_lines = 0\n",
        "    json_errors = 0\n",
        "    missing_key_errors = 0\n",
        "    error_details = []\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            for line_num, line in enumerate(f, 1):\n",
        "                total_lines += 1\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    data = json.loads(line)\n",
        "                    if not required_keys.issubset(data.keys()):\n",
        "                        missing_key_errors += 1\n",
        "                        missing = required_keys - data.keys()\n",
        "                        error_details.append(f\"Line {line_num}: Missing keys {missing}\")\n",
        "                    else:\n",
        "                        valid_lines += 1\n",
        "                except json.JSONDecodeError:\n",
        "                    json_errors += 1\n",
        "                    error_details.append(f\"Line {line_num}: JSON Decode Error\")\n",
        "\n",
        "        print(f\"  Total lines: {total_lines}\")\n",
        "        print(f\"  Valid entries: {valid_lines}\")\n",
        "        print(f\"  JSON errors: {json_errors}\")\n",
        "        print(f\"  Missing key errors: {missing_key_errors}\")\n",
        "        if error_details:\n",
        "            print(\"  Error Details (first 10):\")\n",
        "            for err in error_details[:10]:\n",
        "                print(f\"    {err}\")\n",
        "            if len(error_details) > 10:\n",
        "                print(f\"    ... and {len(error_details) - 10} more errors.\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"  Error: File {file_path} not found.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d38e1309"
      },
      "source": [
        "**Reasoning**:\n",
        "I will script a content analysis to count specific 'AI tells' and 'Marketing Frameworks' phrases within the 'output' field of valid JSON entries in both files. This quantitative comparison will help determine the quality of the generated text and inform the data merging strategy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20fbc82e"
      },
      "source": [
        "import json\n",
        "\n",
        "ai_tells = ['As an AI', 'Certainly', 'Here is', 'limitations']\n",
        "marketing_frameworks = ['ROAS', 'CPA', 'Contribution Margin', 'guardrails']\n",
        "\n",
        "file_paths = ['/content/chatgpt_raw.jsonl', '/content/claude_raw.jsonl']\n",
        "results = {}\n",
        "\n",
        "for file_path in file_paths:\n",
        "    file_name = file_path.split('/')[-1]\n",
        "    results[file_name] = {\n",
        "        'AI Tells': {phrase: 0 for phrase in ai_tells},\n",
        "        'Marketing Frameworks': {phrase: 0 for phrase in marketing_frameworks},\n",
        "        'Total Outputs Analyzed': 0\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    data = json.loads(line)\n",
        "                    if 'output' in data:\n",
        "                        text = data['output']\n",
        "                        results[file_name]['Total Outputs Analyzed'] += 1\n",
        "\n",
        "                        # Count AI Tells (case-insensitive)\n",
        "                        for phrase in ai_tells:\n",
        "                            if phrase.lower() in text.lower():\n",
        "                                results[file_name]['AI Tells'][phrase] += 1\n",
        "\n",
        "                        # Count Marketing Frameworks (case-insensitive)\n",
        "                        for phrase in marketing_frameworks:\n",
        "                            if phrase.lower() in text.lower():\n",
        "                                results[file_name]['Marketing Frameworks'][phrase] += 1\n",
        "                except json.JSONDecodeError:\n",
        "                    continue # Skip malformed lines\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "\n",
        "# Display Results\n",
        "print(\"Phrase Frequency Analysis:\")\n",
        "for file_name, stats in results.items():\n",
        "    print(f\"\\nFile: {file_name}\")\n",
        "    print(f\"  Valid Entries Analyzed: {stats['Total Outputs Analyzed']}\")\n",
        "    print(\"  AI Tells:\")\n",
        "    for phrase, count in stats['AI Tells'].items():\n",
        "        print(f\"    - '{phrase}': {count}\")\n",
        "    print(\"  Marketing Frameworks:\")\n",
        "    for phrase, count in stats['Marketing Frameworks'].items():\n",
        "        print(f\"    - '{phrase}': {count}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "543d2fb2"
      },
      "source": [
        "import json\n",
        "\n",
        "input_files = ['/content/chatgpt_raw.jsonl', '/content/claude_raw.jsonl']\n",
        "output_file = '/content/merged_data.jsonl'\n",
        "\n",
        "merged_data = []\n",
        "total_errors = 0\n",
        "\n",
        "print(\"Starting merge process...\")\n",
        "\n",
        "for file_path in input_files:\n",
        "    print(f\"Processing {file_path}...\")\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line_num, line in enumerate(f, 1):\n",
        "            line = line.strip()\n",
        "            if not line: continue\n",
        "\n",
        "            try:\n",
        "                entry = json.loads(line)\n",
        "                # validation check (optional, based on previous step)\n",
        "                if all(k in entry for k in ('instruction', 'input', 'output')):\n",
        "                    merged_data.append(entry)\n",
        "            except json.JSONDecodeError:\n",
        "                total_errors += 1\n",
        "                # Silently skip errors as per strategy, or print if needed\n",
        "                pass\n",
        "\n",
        "# Save the merged data\n",
        "with open(output_file, 'w') as f:\n",
        "    for entry in merged_data:\n",
        "        f.write(json.dumps(entry) + '\\n')\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\"Merge complete.\")\n",
        "print(f\"Total valid entries merged: {len(merged_data)}\")\n",
        "print(f\"Total lines skipped due to errors: {total_errors}\")\n",
        "print(f\"Merged file saved to: {output_file}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "536a298b"
      },
      "source": [
        "### Findings and Data Merging Strategy\n",
        "\n",
        "**Validation Status:**\n",
        "- **ChatGPT Data**: Found 3 malformed lines (JSON Decode Errors) out of 40 total lines. 37 entries are valid.\n",
        "- **Claude Data**: All 38 lines are valid JSON entries.\n",
        "\n",
        "**Content Analysis:**\n",
        "- **AI Tells**: Both datasets are clean, showing 0 instances of common AI refusal or conversational fillers like \"As an AI\".\n",
        "- **Domain Relevance**: Both datasets successfully incorporate marketing frameworks. ChatGPT has a slightly higher density of \"CPA\" and \"guardrails\" references, while Claude emphasizes \"ROAS\".\n",
        "\n",
        "**Merging Strategy:**\n",
        "1. **Filter**: During the merge process, explicitly handle `json.JSONDecodeError` to skip the 3 corrupt lines in `chatgpt_raw.jsonl`.\n",
        "2. **Combine**: Concatenate the 37 valid ChatGPT entries with the 38 Claude entries.\n",
        "3. **Post-Processing**: Since \"AI tells\" are absent, no aggressive text filtering is required for these specific phrases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8d8b073"
      },
      "source": [
        "### Findings and Data Merging Strategy\n",
        "\n",
        "**Validation Status:**\n",
        "- **ChatGPT Data**: Found 3 malformed lines (JSON Decode Errors) out of 40 total lines. 37 entries are valid.\n",
        "- **Claude Data**: All 38 lines are valid JSON entries.\n",
        "\n",
        "**Content Analysis:**\n",
        "- **AI Tells**: Both datasets are clean, showing 0 instances of common AI refusal or conversational fillers like \"As an AI\".\n",
        "- **Domain Relevance**: Both datasets successfully incorporate marketing frameworks. ChatGPT has a slightly higher density of \"CPA\" and \"guardrails\" references, while Claude emphasizes \"ROAS\".\n",
        "\n",
        "**Merging Strategy:**\n",
        "1. **Filter**: During the merge process, explicitly handle `json.JSONDecodeError` to skip the 3 corrupt lines in `chatgpt_raw.jsonl`.\n",
        "2. **Combine**: Concatenate the 37 valid ChatGPT entries with the 38 Claude entries.\n",
        "3. **Post-Processing**: Since \"AI tells\" are absent, no aggressive text filtering is required for these specific phrases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5bcf72e"
      },
      "source": [
        "### Findings and Data Merging Strategy\n",
        "\n",
        "**Validation Status:**\n",
        "- **ChatGPT Data**: Found 3 malformed lines (JSON Decode Errors) out of 40 total lines. 37 entries are valid.\n",
        "- **Claude Data**: All 38 lines are valid JSON entries.\n",
        "\n",
        "**Content Analysis:**\n",
        "- **AI Tells**: Both datasets are clean, showing 0 instances of common AI refusal or conversational fillers like \"As an AI\".\n",
        "- **Domain Relevance**: Both datasets successfully incorporate marketing frameworks. ChatGPT has a slightly higher density of \"CPA\" and \"guardrails\" references, while Claude emphasizes \"ROAS\".\n",
        "\n",
        "**Merging Strategy:**\n",
        "1. **Filter**: During the merge process, explicitly handle `json.JSONDecodeError` to skip the 3 corrupt lines in `chatgpt_raw.jsonl`.\n",
        "2. **Combine**: Concatenate the 37 valid ChatGPT entries with the 38 valid Claude entries.\n",
        "3. **Post-Processing**: Since \"AI tells\" are absent, no aggressive text filtering is required for these specific phrases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbb2c3bd"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "\n",
        "**Q: What is the validation status of the provided JSONL datasets?**\n",
        "A: The `/content/claude_raw.jsonl` file is perfectly structured, with all 38 lines being valid JSON entries containing the required keys. However, `/content/chatgpt_raw.jsonl` contains data corruption: out of 40 total lines, 3 lines triggered JSON decode errors, resulting in 37 valid entries.\n",
        "\n",
        "**Q: What is the recommended data merging strategy based on the content analysis?**\n",
        "A: The analysis suggests a \"Filter and Combine\" strategy. Specifically, the 3 malformed lines in the ChatGPT dataset should be discarded using exception handling. The remaining 37 valid ChatGPT entries should be concatenated with the 38 Claude entries to form a final dataset of 75 examples. No aggressive text filtering is needed for \"AI tells\" as none were detected.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Structure Validation**:\n",
        "    *   **ChatGPT Data**: 37 out of 40 lines (92.5%) are valid. 3 lines failed due to `json.JSONDecodeError`.\n",
        "    *   **Claude Data**: 38 out of 38 lines (100%) are valid and contain the required keys ('instruction', 'input', 'output').\n",
        "*   **Phrase Frequency - AI Tells**:\n",
        "    *   Both datasets are devoid of common AI refusal or conversational filler phrases (e.g., \"As an AI\", \"Certainly\", \"Here is\"), showing a count of 0 for all tracked terms.\n",
        "*   **Phrase Frequency - Marketing Frameworks**:\n",
        "    *   Both datasets successfully integrate domain specific terms. The ChatGPT data shows a higher prevalence of \"CPA\" and \"guardrails\", whereas the Claude data favors \"ROAS\".\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   **Pipeline Adjustment**: The data ingestion pipeline for ChatGPT outputs requires investigation to identify the source of the JSON corruption affecting 7.5% of the data.\n",
        "*   **Dataset Consolidation**: Proceed immediately with merging the 75 clean records, as the absence of \"AI tells\" implies the text data is high-quality and ready for downstream tasks without further cleaning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5e790ee"
      },
      "source": [
        "# Task\n",
        "Add a text cell to explicitly explain the 'JSON Decode Error' logs observed during the merge process. Clarify that 3 lines in the ChatGPT dataset were malformed and skipped, and confirm that the merge process successfully completed with the remaining 75 entries stored in \"/content/merged_data.jsonl\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3cdd69f"
      },
      "source": [
        "## Clarify Data Validation Results\n",
        "\n",
        "### Subtask:\n",
        "Add a text cell to explain the validation errors and successful merge.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eddb5f91"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "**Q: What caused the 'JSON Decode Error' logs during the merge process?**\n",
        "A: The errors were caused by 3 specific lines in the ChatGPT dataset that were malformed. These lines were identified and skipped to prevent the process from failing.\n",
        "\n",
        "**Q: Was the merged file successfully created?**\n",
        "A: Yes, the merge process was completed successfully. The resulting file contains the remaining 75 valid entries and is stored in `/content/merged_data.jsonl`.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   **Error Identification:** The validation process detected **3** malformed lines in the ChatGPT input data that resulted in JSON decode errors.\n",
        "*   **Data Preservation:** These problematic lines were skipped, ensuring the remaining data was not corrupted.\n",
        "*   **Final Output:** The final merged dataset was successfully saved to `/content/merged_data.jsonl` with **75** verified entries.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The dataset is now clean and technically validated, making it safe for downstream tasks such as statistical analysis or model training.\n",
        "*   Future data collection for the ChatGPT source should be monitored for JSON formatting consistency to reduce data loss.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eda7f5cc"
      },
      "source": [
        "# Task\n",
        "Load the `/content/merged_data.jsonl` dataset using the Hugging Face `datasets` library. Define the Alpaca prompt template and a formatting function to structure the 'instruction', 'input', and 'output' fields, ensuring the EOS token is added. Apply this formatting to the dataset to prepare it for training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a9c1794"
      },
      "source": [
        "## Load and Format Data\n",
        "\n",
        "### Subtask:\n",
        "Load the merged dataset and prepare it for training by applying the Alpaca formatting template.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "881c1559"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the merged dataset, define the Alpaca formatting function with the tokenizer's EOS token, and apply the formatting to prepare the data for training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0027175"
      },
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"json\", data_files=\"/content/merged_data.jsonl\", split=\"train\")\n",
        "\n",
        "# Define the Alpaca prompt template\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "# Retrieve the EOS_TOKEN from the tokenizer (assumed to be defined in previous cells)\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # Format the prompt and append the EOS_TOKEN\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "\n",
        "# Apply the formatting function\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "226670aa"
      },
      "source": [
        "**Reasoning**:\n",
        "Verify the formatting by printing the first element of the processed dataset to ensure the 'text' field is correctly structured with the Alpaca template and EOS token.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c032bcd"
      },
      "source": [
        "print(dataset[0][\"text\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8c59625"
      },
      "source": [
        "## Execute Training\n",
        "\n",
        "### Subtask:\n",
        "Initialize the SFTTrainer with the prepared dataset and training parameters, then run the training process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d899fa9e"
      },
      "source": [
        "**Reasoning**:\n",
        "Initialize the SFTTrainer with the specified configuration and dataset, then execute the training process.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee66778c"
      },
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "import torch\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    packing = False,\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.001,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer_stats = trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22123fa5"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution produced excessive output which might have been flagged as an error or interruption. I will provide the correct code to initialize the trainer and run the training, ensuring all parameters are set as specified in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07e199d0"
      },
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "import torch\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    packing = False,\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.001,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer_stats = trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "424846c0"
      },
      "source": [
        "**Reasoning**:\n",
        "I will initialize the SFTTrainer with the specific hyperparameters provided in the instructions (batch size, learning rate, max steps, etc.) and the formatted dataset, then execute the training process to fine-tune the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3822ac67"
      },
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "import torch\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    packing = False,\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.001,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer_stats = trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bO0PswWX4i13"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c05537a7"
      },
      "source": [
        "**Reasoning**:\n",
        "I will initialize the SFTTrainer with the specific hyperparameters provided in the instructions (batch size, learning rate, max steps, etc.) and the formatted dataset, then execute the training process to fine-tune the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eac614cc"
      },
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "import torch\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    packing = False,\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.001,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer_stats = trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d570eaa"
      },
      "source": [
        "# Task\n",
        "List the contents of the \"/content/outputs/checkpoint-60\" directory to verify that files such as `adapter_model.safetensors` and `trainer_state.json` exist. Then, load the `trainer_state.json` file to create a line plot of the training loss over the recorded steps. Finally, summarize the training status and confirm if the model is ready for inference based on the presence of the checkpoint files and the loss curve."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63cd284f"
      },
      "source": [
        "## Verify Training Completion\n",
        "\n",
        "### Subtask:\n",
        "List the contents of the output directory to verify that critical checkpoint files exist.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20903f9a"
      },
      "source": [
        "**Reasoning**:\n",
        "I will verify the existence of critical training artifacts in the specified checkpoint directory to confirm that the training process successfully saved the model and state.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6f6233c"
      },
      "source": [
        "import os\n",
        "\n",
        "output_dir = \"/content/outputs/checkpoint-60\"\n",
        "\n",
        "if os.path.exists(output_dir):\n",
        "    files = os.listdir(output_dir)\n",
        "    print(f\"Files in {output_dir}:\\n{files}\\n\")\n",
        "\n",
        "    required_files = [\"adapter_model.safetensors\", \"trainer_state.json\"]\n",
        "    for file in required_files:\n",
        "        if file in files:\n",
        "            print(f\"CONFIRMED: {file} exists.\")\n",
        "        else:\n",
        "            print(f\"MISSING: {file} not found.\")\n",
        "else:\n",
        "    print(f\"Directory not found: {output_dir}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "497f57f9"
      },
      "source": [
        "**Reasoning**:\n",
        "I will load the 'trainer_state.json' file, extract the training loss history, and plot the loss curve over the training steps to visualize the model's convergence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18f6610d"
      },
      "source": [
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "trainer_state_path = \"/content/outputs/checkpoint-60/trainer_state.json\"\n",
        "\n",
        "with open(trainer_state_path, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "log_history = data.get(\"log_history\", [])\n",
        "\n",
        "steps = []\n",
        "losses = []\n",
        "\n",
        "for entry in log_history:\n",
        "    if \"loss\" in entry and \"step\" in entry:\n",
        "        steps.append(entry[\"step\"])\n",
        "        losses.append(entry[\"loss\"])\n",
        "\n",
        "if steps and losses:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(steps, losses, marker='o', linestyle='-', color='b')\n",
        "    plt.title(\"Training Loss Over Steps\")\n",
        "    plt.xlabel(\"Step\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No loss data found in log_history.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b76955be"
      },
      "source": [
        "### Training Summary\n",
        "\n",
        "*   **Checkpoint Verification**: Successfully validated the presence of `adapter_model.safetensors` and `trainer_state.json` in the `/content/outputs/checkpoint-60` directory.\n",
        "*   **Training Loss**: The loss curve demonstrates a consistent downward trend, decreasing from an initial value of approximately 0.26 to around 0.04 by step 60. This indicates that the model has effectively converged and learned from the dataset.\n",
        "*   **Status**: The model fine-tuning is complete, the artifacts are saved, and the model is ready for inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26d921d1"
      },
      "source": [
        "**Reasoning**:\n",
        "I will verify the existence of critical training artifacts in the specified checkpoint directory to confirm that the training process successfully saved the model and state.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46f01fd8"
      },
      "source": [
        "import os\n",
        "\n",
        "output_dir = \"/content/outputs/checkpoint-60\"\n",
        "\n",
        "if os.path.exists(output_dir):\n",
        "    files = os.listdir(output_dir)\n",
        "    print(f\"Files in {output_dir}:\\n{files}\\n\")\n",
        "\n",
        "    required_files = [\"adapter_model.safetensors\", \"trainer_state.json\"]\n",
        "    for file in required_files:\n",
        "        if file in files:\n",
        "            print(f\"CONFIRMED: {file} exists.\")\n",
        "        else:\n",
        "            print(f\"MISSING: {file} not found.\")\n",
        "else:\n",
        "    print(f\"Directory not found: {output_dir}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d14d29ef"
      },
      "source": [
        "## Inference\n",
        "\n",
        "### Subtask:\n",
        "Run the fine-tuned model to generate a response for a test input.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3dfc76c"
      },
      "source": [
        "**Reasoning**:\n",
        "Enable inference mode on the model, prepare the test input using the alpaca prompt template, tokenize it, generate a response, and then decode and print the output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "299a302b"
      },
      "source": [
        "# Save the LoRA adapters and tokenizer locally\n",
        "model.save_pretrained(\"lora_model\")\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "\n",
        "print(\"Model adapters and tokenizer saved to 'lora_model' directory.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "da5b3889"
      },
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Enable native 2x faster inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Define input using the alpaca_prompt format\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"Continue the fibonnaci sequence.\", # instruction\n",
        "        \"1, 1, 2, 3, 5, 8\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate response\n",
        "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
        "\n",
        "# Decode and print\n",
        "print(tokenizer.batch_decode(outputs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf5e1167"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the verification results and confirm the model is ready for inference.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "513c0644"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "**Q:** Are the required checkpoint files available for inference?\n",
        "**A:** Yes, the contents of the `/content/outputs/checkpoint-60` directory were verified, confirming the presence of essential files such as `adapter_model.safetensors` and `trainer_state.json`.\n",
        "\n",
        "**Q:** Did the model training converge successfully?\n",
        "**A:** Yes, the analysis of the training log history indicates successful convergence, as the loss consistently decreased over the recorded steps.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   **Artifact Verification**: The output directory `/content/outputs/checkpoint-60` contains all necessary model artifacts, including `adapter_model.safetensors`, `trainer_state.json`, `tokenizer.json`, and optimizer states.\n",
        "*   **Loss Analysis**: The training loss demonstrated a clear downward trajectory, starting at approximately **0.26** and dropping to around **0.04** by step 60, indicating the model effectively learned from the training data.\n",
        "*   **Functional Inference**: The model successfully processed a test prompt regarding the Fibonacci sequence (\"1, 1, 2, 3, 5, 8\"), correctly generating the next number in the sequence (**13**) and providing a coherent textual explanation.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The consistent drop in loss combined with the successful inference test confirms that the fine-tuning process was stable and the model is technically sound.\n",
        "*   The model is now ready for more extensive testing; future steps should involve evaluating the model against a diverse set of prompts to ensure robustness beyond the initial validation case.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db9f1c8a"
      },
      "source": [
        "# Task\n",
        "Save the fine-tuned model to GGUF format using `model.save_pretrained_gguf` with the `q4_k_m` quantization method, storing it in the \"model\" directory. Then, list the contents of the \"model\" directory to verify the presence of the `.gguf` file. Finally, confirm the conversion is complete."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "745d68be"
      },
      "source": [
        "## Convert to GGUF\n",
        "\n",
        "### Subtask:\n",
        "Save the model in GGUF format with q4_k_m quantization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84443b4a"
      },
      "source": [
        "**Reasoning**:\n",
        "I will execute the command to save the model in GGUF format with q4_k_m quantization as requested in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "654aee93"
      },
      "source": [
        "# Save the model in GGUF format with q4_k_m quantization\n",
        "model.save_pretrained_gguf(\"model\", tokenizer, quantization_method=\"q4_k_m\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b79dde37"
      },
      "source": [
        "# Task\n",
        "List the contents of the \"model\" directory to verify that the GGUF file has been successfully created. Finally, print a confirmation message that the GGUF model is ready for use.\n",
        "# Plan:\n",
        "* **Verify GGUF Output**: List the contents of the \"model\" directory to confirm that the `.gguf` file exists. This verifies that the conversion and saving process completed successfully.\n",
        "* **Final Task**: Confirm the GGUF model is ready for use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27da2553"
      },
      "source": [
        "## Verify GGUF Output\n",
        "\n",
        "### Subtask:\n",
        "Verify that the GGUF model file was successfully created in the 'model' directory.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e08adee"
      },
      "source": [
        "**Reasoning**:\n",
        "I will verify the creation of the GGUF file by listing the contents of the 'model' directory and checking for any file with the '.gguf' extension.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49f579a3"
      },
      "source": [
        "# Task\n",
        "Mount the user's Google Drive to `/content/drive`. Create a backup folder named `DigitalStrategist_Backup` in the drive. Copy the `/content/merged_data.jsonl` file, the `/content/lora_model` directory, and the `/content/model` directory into the backup folder. Finally, confirm the backup is complete and provide a brief instruction on how to load these files back into the environment when resuming work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3d2aa91"
      },
      "source": [
        "## Mount Google Drive\n",
        "\n",
        "### Subtask:\n",
        "Mount the user's Google Drive to `/content/drive` to enable persistent file storage.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "601f1569"
      },
      "source": [
        "**Reasoning**:\n",
        "I will mount the Google Drive to allow access to persistent storage as requested.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "321e0996"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aff62b5"
      },
      "source": [
        "**Reasoning**:\n",
        "I will create a backup directory on Google Drive and copy the specified files and directories into it to ensure data persistence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b67abf6a"
      },
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Define source paths\n",
        "source_file = '/content/merged_data.jsonl'\n",
        "source_dirs = ['/content/lora_model', '/content/model']\n",
        "\n",
        "# Define destination path\n",
        "backup_dir = '/content/drive/My Drive/DigitalStrategist_Backup'\n",
        "\n",
        "# Create backup directory if it doesn't exist\n",
        "if not os.path.exists(backup_dir):\n",
        "    os.makedirs(backup_dir)\n",
        "    print(f\"Created backup directory: {backup_dir}\")\n",
        "else:\n",
        "    print(f\"Backup directory already exists: {backup_dir}\")\n",
        "\n",
        "# Copy file\n",
        "dest_file = os.path.join(backup_dir, os.path.basename(source_file))\n",
        "shutil.copy2(source_file, dest_file)\n",
        "print(f\"Copied {source_file} to {dest_file}\")\n",
        "\n",
        "# Copy directories\n",
        "for source_dir in source_dirs:\n",
        "    dir_name = os.path.basename(source_dir)\n",
        "    dest_dir = os.path.join(backup_dir, dir_name)\n",
        "    if os.path.exists(dest_dir):\n",
        "        shutil.rmtree(dest_dir)  # Remove existing backup of the directory to ensure clean copy\n",
        "    shutil.copytree(source_dir, dest_dir)\n",
        "    print(f\"Copied {source_dir} to {dest_dir}\")\n",
        "\n",
        "print(\"\\nBackup complete!\")\n",
        "print(\"To load these files back in a future session:\")\n",
        "print(\"1. Mount Google Drive.\")\n",
        "print(f\"2. Copy files from '{backup_dir}' back to '/content/'.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Replace with your notebook's name\n",
        "filename = \"Llama3.1_(8B)-Alpaca.ipynb\"\n",
        "\n",
        "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "    nb = json.load(f)\n",
        "\n",
        "# Delete the problematic widgets metadata\n",
        "if \"widgets\" in nb.get(\"metadata\", {}):\n",
        "    del nb[\"metadata\"][\"widgets\"]\n",
        "    print(\"Fixed: Removed problematic widget metadata.\")\n",
        "\n",
        "with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(nb, f, indent=2)"
      ],
      "metadata": {
        "id": "KkdKFO1CFMBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e79bb32a"
      },
      "source": [
        "# Task\n",
        "List the files and directories in `/content/drive/My Drive/DigitalStrategist_Backup` to verify the backup status. After listing, report which files are safely backed up and which might need to be re-copied."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e6bc1c6"
      },
      "source": [
        "## Verify Backup Contents\n",
        "\n",
        "### Subtask:\n",
        "List the files and directories in the backup folder to assess the integrity of the backup following the previous interruption.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c283349"
      },
      "source": [
        "**Reasoning**:\n",
        "I will write a script to inspect the backup directory on Google Drive, listing files and checking the contents of subdirectories to determine which parts of the backup were successful before the interruption.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a234b996"
      },
      "source": [
        "import os\n",
        "\n",
        "backup_dir = '/content/drive/My Drive/DigitalStrategist_Backup'\n",
        "\n",
        "print(f\"Checking backup directory: {backup_dir}\")\n",
        "\n",
        "if os.path.exists(backup_dir):\n",
        "    contents = os.listdir(backup_dir)\n",
        "    print(f\"Contents of backup directory:\\n{contents}\\n\")\n",
        "\n",
        "    # Check for specific items\n",
        "    expected_items = ['merged_data.jsonl', 'lora_model', 'model']\n",
        "    for item in expected_items:\n",
        "        item_path = os.path.join(backup_dir, item)\n",
        "        if item in contents:\n",
        "            if os.path.isdir(item_path):\n",
        "                sub_contents = os.listdir(item_path)\n",
        "                print(f\"Found directory '{item}' with {len(sub_contents)} files:\")\n",
        "                print(sub_contents)\n",
        "            else:\n",
        "                print(f\"Found file: {item}\")\n",
        "        else:\n",
        "            print(f\"MISSING: {item}\")\n",
        "else:\n",
        "    print(f\"Backup directory not found at: {backup_dir}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# This fetches the name of the current notebook session\n",
        "filename = requests.get(\"http://172.28.0.12:9000/api/sessions\").json()[0][\"name\"]\n",
        "\n",
        "print(f\"Your notebook name is: {filename}\")"
      ],
      "metadata": {
        "id": "YM29O3fVKXr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Replace with your notebook's name\n",
        "filename = \"Llama3.1_(8B)-Alpaca.ipynb\"\n",
        "\n",
        "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "    nb = json.load(f)\n",
        "\n",
        "# Delete the problematic widgets metadata\n",
        "if \"widgets\" in nb.get(\"metadata\", {}):\n",
        "    del nb[\"metadata\"][\"widgets\"]\n",
        "    print(\"Fixed: Removed problematic widget metadata.\")\n",
        "\n",
        "with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(nb, f, indent=2)"
      ],
      "metadata": {
        "id": "eFIwR2xRKjak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "\n",
        "# 1. Automatically find the name of the notebook you are currently in\n",
        "try:\n",
        "    # Use the Colab internal API to get the current notebook filename\n",
        "    filename = requests.get(\"http://172.28.0.12:9000/api/sessions\").json()[0][\"name\"]\n",
        "    if not filename.endswith(\".ipynb\"):\n",
        "        filename += \".ipynb\"\n",
        "except:\n",
        "    # Fallback: If you renamed it manually, it might be in /content/\n",
        "    filename = \"Llama3.1_(8B)-Alpaca.ipynb\"\n",
        "\n",
        "print(f\"Targeting notebook: {filename}\")\n",
        "\n",
        "# 2. Check if the file exists in the local directory.\n",
        "# If not, we have to save a local copy first to \"edit\" it.\n",
        "if not os.path.exists(filename):\n",
        "    print(\"Notebook file not found locally. Saving a temporary copy...\")\n",
        "    # This creates a physical file of your current progress so the script can edit it\n",
        "    from google.colab import _message\n",
        "    _message.blocking_request('save_ipynb', request={'path': filename}, timeout_sec=30)\n",
        "\n",
        "# 3. Clean the metadata\n",
        "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "    nb = json.load(f)\n",
        "\n",
        "if \"widgets\" in nb.get(\"metadata\", {}):\n",
        "    del nb[\"metadata\"][\"widgets\"]\n",
        "    print(\"Success: Removed problematic 'widgets' metadata.\")\n",
        "else:\n",
        "    print(\"Metadata already clean or 'widgets' key not found.\")\n",
        "\n",
        "# 4. Save the 'Fixed' version\n",
        "with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(nb, f, indent=2)\n",
        "\n",
        "print(f\"DONE! You can now download '{filename}' from the folder icon on the left and upload it to GitHub.\")"
      ],
      "metadata": {
        "id": "ern6CWFELPvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from google.colab import _message\n",
        "\n",
        "# 1. Force Colab to save the current notebook to the local disk\n",
        "filename = \"Digital_Strategist_Final.ipynb\"\n",
        "print(f\"Saving current progress to {filename}...\")\n",
        "\n",
        "# This command physically creates the file in your /content/ folder\n",
        "_message.blocking_request('save_ipynb', request={'path': filename}, timeout_sec=30)\n",
        "\n",
        "# 2. Open the file we just created and strip the broken 'widgets' metadata\n",
        "if os.path.exists(filename):\n",
        "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "        nb = json.load(f)\n",
        "\n",
        "    # Check for the problematic 'widgets' key\n",
        "    if \"widgets\" in nb.get(\"metadata\", {}):\n",
        "        del nb[\"metadata\"][\"widgets\"]\n",
        "        print(\"✅ Success: Problematic 'widgets' metadata removed.\")\n",
        "    else:\n",
        "        print(\"ℹ️ Metadata was already clean.\")\n",
        "\n",
        "    # 3. Save the fixed version back to the disk\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(nb, f, indent=2)\n",
        "\n",
        "    print(f\"🚀 DONE! '{filename}' is ready.\")\n",
        "else:\n",
        "    print(\"❌ Error: Could not create the local file. Try the 'Manual Fix' below.\")"
      ],
      "metadata": {
        "id": "Xq0yxnsFLsC0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}